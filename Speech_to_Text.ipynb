{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinandanRoy7/Project/blob/main/Speech_to_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EXstK9R-oG3"
      },
      "outputs": [],
      "source": [
        "!pip install yt-dlp\n",
        "!pip install librosa soundfile\n",
        "!pip install transformers torch\n",
        "!pip install librosa soundfile transformers torch noisereduce\n",
        "!pip install pyctcdecode\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA9wovPc-sVG"
      },
      "outputs": [],
      "source": [
        "!yt-dlp -x --audio-format wav https://www.youtube.com/shorts/oyPn7DnRVqY -o \"audio.%(ext)s\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "g157_HZX_C3a"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import noisereduce as nr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dnUUPMPBAKC8"
      },
      "outputs": [],
      "source": [
        "audio_file = \"audio.wav\"\n",
        "speech, sr = librosa.load(audio_file, sr=16000)  # Resample to 16kHz\n",
        "\n",
        "# If the audio is stereo, convert it to mono\n",
        "if speech.ndim > 1:\n",
        "    speech = librosa.to_mono(speech)\n",
        "\n",
        "\n",
        "# Load audio\n",
        "audio, sr = librosa.load('audio.wav', sr=16000)\n",
        "\n",
        "# Reduce noise\n",
        "reduced_noise_audio = nr.reduce_noise(y=audio, sr=sr)\n",
        "\n",
        "# Save the processed audio\n",
        "sf.write('processed_audio.wav', speech, sr)\n",
        "\n",
        "\n",
        "# Save the preprocessed audio\n",
        "sf.write(\"processed_audio.wav\", speech, 16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1lgWSKgF_F7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch librosa\n",
        "import transformers\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torch\n",
        "import librosa\n",
        "\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Librosa version:\", librosa.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XIem55RXkcBt"
      },
      "outputs": [],
      "source": [
        "kenlm_model_path=\"path/to/language_model.binary\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1gyatX3AOrP"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch librosa\n",
        "\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torch\n",
        "import librosa\n",
        "\n",
        "\n",
        "# Load pre-trained model and processor from Hugging Face\n",
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "try:\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "except Exception as e:\n",
        "    print(\"Error loading model or processor:\", e)\n",
        "\n",
        "# Load the preprocessed audio file\n",
        "speech, sr = librosa.load(\"processed_audio.wav\", sr=16000)\n",
        "\n",
        "# Trim the audio to the first 10 seconds (or any desired length)\n",
        "max_length = 10\n",
        "max_samples = max_length * sr\n",
        "speech = speech[:max_samples]  # Trimming the audio\n",
        "\n",
        "# Process the audio input for the model\n",
        "input_values = processor(speech, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "\n",
        "# Check if input values are not too large\n",
        "print(f\"Input values shape: {input_values.shape}\")\n",
        "\n",
        "# Perform inference (forward pass) with error handling\n",
        "try:\n",
        "    with torch.no_grad():  # Avoid storing gradients\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Get the predicted tokens\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode the predicted tokens into text (transcript)\n",
        "    transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "    # Print the transcript\n",
        "    print(\"Transcript: \", transcription)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during inference:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zLhdTVCIKNZz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to split audio into chunks (e.g., 10 seconds)\n",
        "def split_audio(audio, sr, chunk_length=10):\n",
        "    chunk_size = chunk_length * sr\n",
        "    return [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]\n",
        "\n",
        "# Split audio into 10-second chunks\n",
        "chunks = split_audio(speech, sr, chunk_length=10)\n",
        "\n",
        "# Transcribe each chunk\n",
        "transcriptions = []\n",
        "for chunk in chunks:\n",
        "    input_values = processor(chunk, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "    logits = model(input_values).logits\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.decode(predicted_ids[0])\n",
        "    transcriptions.append(transcription)\n",
        "\n",
        "# Combine the transcriptions\n",
        "full_transcript = \" \".join(transcriptions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC3rNN7MAt2N",
        "outputId": "cdd74d68-3c81-43d4-a78d-73aec65ac79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 1/3\n",
            "Processed chunk 2/3\n",
            "Processed chunk 3/3\n",
            "Full Transcript:  THIS IS DASH AND HE'S AN ARCHER TRAINING HIS PULL BACK STRENGTH WITH HIS BOW THE AMOUNT OF FORCE NEEDED TO PULL BACK BOWS LIKE THIS ARE A LOT SO TRAINING IS NECESSARY AND HERE DASH IS SHOWING HOW MUCH POUNDS OF FORCE THIS BOW SHES SHOWING HOW MUCH POUNDS OF FORCE THIS BEAW REALLY IS COMING IN AT ONE HUNDRED FIFTY EIGHT POUNDS AND HEAR'S WHAT HE LOOKS LIKE PULLING BACK THAT SAME EXACT BOW MAYBE BEING AN ARCHER IS THE BEST WAY TO GET A PHYSIQUE LIKE THAT THE BEST WAY TO GET A PHYSIQUE LIKE THAT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "# Load pre-trained model and processor from Hugging Face\n",
        "model_name = \"facebook/wav2vec2-base-960h\"  # Use a small model for efficiency\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "\n",
        "# Load the preprocessed audio file\n",
        "speech, sr = librosa.load(\"processed_audio.wav\", sr=16000)\n",
        "\n",
        "# Function to split audio into chunks with overlap\n",
        "def split_audio(audio, sr, chunk_length=10, overlap=2):\n",
        "    chunk_size = chunk_length * sr\n",
        "    overlap_size = overlap * sr\n",
        "    chunks = []\n",
        "    for i in range(0, len(audio), chunk_size - overlap_size):\n",
        "        chunks.append(audio[i:i + chunk_size])\n",
        "    return chunks\n",
        "\n",
        "# Split audio into 10-second chunks with 2 seconds of overlap\n",
        "chunks = split_audio(speech, sr, chunk_length=10, overlap=2)\n",
        "\n",
        "# Transcribe each chunk\n",
        "transcriptions = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Process the audio input for the model\n",
        "    input_values = processor(chunk, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "\n",
        "    # Perform inference (forward pass)\n",
        "    with torch.no_grad():  # Avoid storing gradients\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Get the predicted tokens\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode the predicted tokens into text (transcript)\n",
        "    transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Processed chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "    transcriptions.append(transcription)\n",
        "\n",
        "# Combine the transcriptions into a single full transcript\n",
        "full_transcript = \" \".join(transcriptions)\n",
        "print(\"Full Transcript: \", full_transcript)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FesAjWV7G9fN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgaBI3aKjnmctXqjCPY0mv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}